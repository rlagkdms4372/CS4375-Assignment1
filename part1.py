# -*- coding: utf-8 -*-
"""part1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XfEZZwhH_ovXD3MPt5g5nI2oWz4apw0t
"""

# The libraries that I used for the part 1
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

# Bring out the dataset 
mpg = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data-original", sep='\s+', header = None)

# Rename the Column from the dataset, and delete the last column which is the name of car
mpg.columns = ['mpg', 'cylinders','displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', 'car_name']
mpg = mpg.drop(['car_name'], axis = 1)
# Standarization of dataset
scaler=StandardScaler()
scaler.fit(mpg)
mpg = scaler.transform(mpg)
# Rename the Column
mpg = pd.DataFrame(mpg,columns=['mpg', 'cylinders','displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin'])

# Drop the not suitable columns(Discrete Variables)
mpg = mpg.drop(['origin', 'cylinders', 'model_year'], axis = 1)

# Check out the number of NAs
mpg.isna().sum()

# Remove null or NA values
mpg.dropna(inplace = True)
# Remove any redundant rows
mpg.drop_duplicates(inplace = True)

# Find the information about the dataset, and check the NULLs
mpg.info()

# Making the heatmap plot to check the correlation among the attributes
corr = mpg.corr()
plt.figure(figsize=[10,10])
sns.heatmap(corr,annot=True)

# Making the scatter plot to check the relationship between mpg and other attributes
attributes=list(mpg.columns)
plt.figure(figsize=[10,10])
plt.subplots_adjust(wspace=0.5)
for i in range(len(attributes)):
    plt.subplot(2,3,i+1)
    sns.scatterplot(data=mpg,x=mpg[attributes[i]],y='mpg')

# The target of this dataset is MPG, so Y is MPG, and X is the others except for Y
Y = mpg['mpg']
X = mpg.drop(['mpg'], axis=1)

# Split the dataset 80(train) : 20(test)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)

# Change the train and test set into dataframe
x_train = pd.DataFrame(X_train)
y_train = pd.DataFrame(Y_train)
x_test = pd.DataFrame(X_test)
y_test = pd.DataFrame(Y_test)

# add the column names axis which is for constant
axis = [1] * len(x_train)
x_train.insert(0, 'axis', axis)
axis = [1] * len(x_test)
x_test.insert(0, 'axis', axis)

# This is the cost function
def cost(data, target, weight):
  m = len(target)
  y_pred = data.dot(weight)
  j_func = (1/(2*m))*np.sum((y_pred - target)**2)
  return j_func

# This is the gradient funcsion
def gradient(data, target, weight, learn_rate, n_iter):
  m = len(target)
  grad_list = [0] * n_iter
  for i in range(n_iter):
    y_pred = pd.DataFrame(data.dot(weight),columns=['mpg'] )
    difference = y_pred - target
    weight = weight - (learn_rate * (1/m)*(data.T.dot(difference)))
    grad_list[i] = cost(data, target, weight)
  return weight, grad_list

theta = pd.DataFrame({'mpg' : [0, 0, 0, 0, 0]}, index = ['axis', 'displacement', 'horsepower', 'weight', 'acceleration'])

theta_pred, grad_list_pred = gradient(x_train, y_train, theta, 0.1, 3000)

# model evaluation for training set
y_train_predict = x_train.dot(theta_pred)
mse = mean_squared_error(y_train, y_train_predict)
r2 = r2_score(y_train, y_train_predict)

print("--------------------------------------")
print("The model performance for train set")
print("--------------------------------------")
print('MSE is {}'.format(round(mse,5)))
print('R2 score is {}'.format(round(r2,5)))
print("--------------------------------------")
print("\n")

# model evaluation for test set
y_test_predict = x_test.dot(theta_pred)
mse = mean_squared_error(y_test, y_test_predict)
r2 = r2_score(y_test, y_test_predict)

print("--------------------------------------")
print("The model performance for test set")
print("--------------------------------------")
print('MSE is {}'.format(round(mse,5)))
print('R2 score is {}'.format(round(r2,5)))
print("--------------------------------------")
print("\n")

# Making the plot for cost vs iteration
plt.plot(range(3000),grad_list_pred)
plt.xlabel('Number of Iterations')
plt.ylabel('Mean Squared Error')
plt.title('Cost vs Iterations')

# Finding out the optimized iteration
np.argmin(grad_list_pred)

# weight coefficients
theta_pred